attach(data)
error <- rep(0,poly)
for (j in 1:poly) {        ## for polynomial
ith_fold_error <- rep(0,k)    ## record error for each iteration of folds
for (i in 1:k) {  ## k-folds
train <- (groups==i)
lm.fit <- lm(mpg~poly(horsepower,j), data = data, subset = train)
ith_fold_error[i] <- mean((mpg-predict(lm.fit,Auto))[-train]^2)   ## Coallate error per fold
}
error[j] <- mean(ith_fold_error)         ## Error of that polynomial = mean(ith_fold_error)
}
return (error)
}
k_foldCV(Auto, 10, 17, 10)
k_foldCV = function(data, k, seed, poly) {
set.seed(seed)
n <- nrow(data)
train_size <- floor(n/k)
groups <- sample(rep(1:k, train_size), n, replace = TRUE)
data <- cbind(data,groups)
attach(data)
error <- rep(0,poly)
for (j in 1:poly) {        ## for polynomial
ith_fold_error <- rep(0,k)    ## record error for each iteration of folds
for (i in 1:k) {  ## k-folds
train <- (groups !=i)
lm.fit <- lm(mpg~poly(horsepower,j), data = data, subset = train)
ith_fold_error[i] <- mean((mpg-predict(lm.fit,Auto))[-train]^2)   ## Coallate error per fold
}
error[j] <- mean(ith_fold_error)         ## Error of that polynomial = mean(ith_fold_error)
}
return (error)
}
k_foldCV(Auto, 10, 17, 10)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
cv.error=rep(0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
k_foldCV(Auto, 10, 17, 10)
Lab <- cv.error.10
Lab <- cv.error.10
Mine <- k_foldCV(Auto, 10, 17, 10)
cbind(Mine, Lab)
Final <- cbind(Mine, Lab)
Final
?glm
?cb.glm
?cv.glm
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(stats)
library(boot)
k_foldCV = function(data, k, seed, poly) {
set.seed(seed)
n <- nrow(data)
train_size <- floor(n/k)
groups <- sample(rep(1:k, train_size), n, replace = TRUE)  ## rep(1:k, train_size) generates each number from 1:k train_size times
data <- cbind(data,groups)       ## Sample then picks from this list until it hits n values, with replacement
## Then we bind it together
attach(data)
error <- rep(0,poly)
for (j in 1:poly) {        ## for polynomial
ith_fold_error <- rep(0,k)    ## record error for each iteration of folds
for (i in 1:k) {  ## k-folds
train <- (groups !=i)
lm.fit <- lm(mpg~poly(horsepower,j), data = data, subset = train)
ith_fold_error[i] <- mean((mpg-predict(lm.fit,Auto))[-train]^2)   ## Coallate error per fold
}
error[j] <- mean(ith_fold_error)         ## Error of that polynomial = mean(ith_fold_error)
}
return (error)
}
Mine <- k_foldCV(Auto, 10, 1, 10)
set.seed(1)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
Lab <- cv.error.10
cbind(Mine, Lab)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(stats)
library(boot)
k_foldCV = function(data, k, poly) {
n <- nrow(data)
train_size <- floor(n/k)
groups <- sample(rep(1:k, train_size), n, replace = TRUE)  ## rep(1:k, train_size) generates each number from 1:k train_size times
data <- cbind(data,groups)       ## Sample then picks from this list until it hits n values, with replacement
## Then we bind it together
attach(data)
error <- rep(0,poly)
for (j in 1:poly) {        ## for polynomial
ith_fold_error <- rep(0,k)    ## record error for each iteration of folds
for (i in 1:k) {  ## k-folds
train <- (groups !=i)
lm.fit <- lm(mpg~poly(horsepower,j), data = data, subset = train)
ith_fold_error[i] <- mean((mpg-predict(lm.fit,Auto))[-train]^2)   ## Coallate error per fold
}
error[j] <- mean(ith_fold_error)         ## Error of that polynomial = mean(ith_fold_error)
}
return (error)
}
Mine <- k_foldCV(Auto, 10, 10)
set.seed(1)
Mine <- k_foldCV(Auto, 10,10)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
Lab <- cv.error.10
cbind(Mine, Lab)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
Lab <- cv.error.10
cbind(Mine, Lab)
install.packages("reveal.js")
install.packages("revealjs")
# install from CRAN
install.packages('xaringan')
# or GitHub
devtools::install_github('yihui/xaringan')
install.packages("devtools")
install.packages("devtools")
shiny::runApp('School/Education/Y3S1/DSA3101/Shiny/DSA3101')
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss.summary$rss, type="l")
reg.ss <- regsubsets(y~., data, nvmax = 10)
## Best subset selection
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss.summary$rss, type="l")
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
x <- rnorm(100)
eps <- rnorm(100)
y <- 1 + 1*x + 1*x^2 + 1*x^3 + eps
X <- c()
for (i in 1:10) {
cur <- x^i
X <- cbind(X,cur)
}
names = c()
for (i in 1:10) {
cur = paste0("x",i)
names = append(names, cur)
}
colnames(X) <- names
# c)
reg.ss <- regsubsets(y~., data, nvmax = 10)
## Best subset selection
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss.summary$rss, type="l")
data <- as.data.frame(cbind(X,y))
reg.ss <- regsubsets(y~., data, nvmax = 10)
regsubsets(y~., data, nvmax = 10)
library(leaps)
reg.ss <- regsubsets(y~., data, nvmax = 10)
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss.summary$r2, type="l")
plot(reg.ss.summary$cp, type="l")
plot(reg.ss.summary$bic, type="l")
dev.off()
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss, scale="r2")
plot(reg.ss.summary$cp, type="l")
plot(reg.ss.summary$bic, type="l")
reg.ss.summary
index(min(reg.ss.summary$bic))
min(reg.ss.summary$bic)
reg.ss.summary$bic
reg.ss.summary$Index
reg.ss.summary$index
which.min(reg.ss.summary$bic)
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss, scale="r2")
plot(reg.ss.summary$cp, type="l")
plot(reg.ss.summary$bic, type="l")
index.ss <- which.min(reg.ss.summary$bic)
which.min(reg.ss.summary$cp)
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss, scale="r2")
plot(reg.ss.summary$cp, type="l")
plot(reg.ss.summary$bic, type="l")
index.ss <- which.min(reg.ss.summary$bic)
## Using which.min(reg.ss.summary$cp) gives same result
points(min(Reg.ss.summary$bic), index.ss, col="Red")
points(min(reg.ss.summary$bic), index.ss, col="Red")
dev.off()
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss, scale="r2")
plot(reg.ss.summary$cp, type="l")
plot(reg.ss.summary$bic, type="l")
index.ss <- which.min(reg.ss.summary$bic)
## Using which.min(reg.ss.summary$cp) gives same result
points(min(reg.ss.summary$bic), index.ss, col="Red")
## Best subset selection
reg.ss.summary <- summary(reg.ss)
par(mfrow=c(2,2))
plot(reg.ss, scale="r2")
plot(reg.ss.summary$cp, type="l")
points(min(reg.ss.summary$bic), index.ss, col="Red")
plot(reg.ss.summary$bic, type="l")
points(min(reg.ss.summary$bic), index.ss, col="Red")
index.ss <- which.min(reg.ss.summary$bic)
## Using which.min(reg.ss.summary$cp) gives same result
points(min(reg.ss.summary$bic), index.ss, col="Red")
points(index.ss, reg.ss.summary$cp[index])
points(index.ss, reg.ss.summary$cp[index.ss])
coef(reg.ss, index.ss)
reg.for <- regsubsets(y~., data, nvmax = 10, method = "forward")
par(mfrow=c(2,2))
reg.for <- regsubsets(y~., data, nvmax = 10, method = "forward")
reg.for.summary <- summary(reg.for)
par(mfrow=c(2,2))
plot(reg.for, scale="r2")
plot(reg.for.summary$cp, type="l")
plot(reg.for.summary$bic, type="l")
plot(reg.for.summary$bic, type="l")
reg.for <- regsubsets(y~., data, nvmax = 10, method = "forward")
reg.for.summary <- summary(reg.for)
par(mfrow=c(2,2))
plot(reg.for, scale="r2")
plot(reg.for.summary$cp, type="l")
plot(reg.for.summary$bic, type="l")
coef(reg.ss, index.ss)
index.for <- which.min(reg.for.summary$bic) ## Using which.min(reg.for.summary$cp) gives same result
coef(reg.for, index.for)
reg.for
which.min(reg.for.summary$cp)
index.for <- which.min(reg.for.summary$bic)
coef(reg.for, index.for)
coef(reg.ss, index.ss)
reg.back <- regsubsets(y~., data, nvmax = 10, method = "backward")
reg.back.summary <- summary(reg.back)
par(mfrow=c(2,2))
plot(reg.back, scale="r2")
plot(reg.back.summary$cp, type="l")
plot(reg.back.summary$bic, type="l")
index.back <- which.min(reg.back.summary$bic) ## Using which.min(reg.for.summary$cp) gives same result
coef(reg.back, index.back)
index.back
which.min(reg.back.summary$cp)
which.min(reg.back.summary$bic
)
data
data
library(shiny)
library(dygraphs)
library(rgdal)
library(flexdashboard)
library(tidyverse)
library(plotly)
library(jsonlite)
library(DT)
library(jsonlite)
library(xts)
library(dplyr)
library(tmap)
##add fake information here first
data = readLines("fakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d-T%H:%M:%S.000Z"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H:%M:%S")
data = rbind(data, temp)
}
## Add count TRY ONLY
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE))
data
setwd("C:/Users/Zoe Lua/dsa3101-2210-03-priv/frontend/aisleHeatMap/Realheatmap")
data = readLines("fakeData.json")
data = readLines("../fakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d-T%H:%M:%S.000Z"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H:%M:%S")
data = rbind(data, temp)
}
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE), cp = sample(c(1:20),nrow(data),replace=TRUE))
data
attach(data)
c = count_vect.fit)
data[data$camera == 1]
data[data$camera == 1,]
data = readLines("../updatedFakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d-T%H:%M:%S.000Z"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H:%M:%S")
data = rbind(data, temp)
}
## Add count TRY ONLY
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE), cp = sample(c(1:20),nrow(data),replace=TRUE))
data[data$camera == 1,]
data = readLines("../updatedFakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d-T%H:%M:%S.000Z"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H:%M:%S")
data = rbind(data, temp)
}
data
##add fake information here first
data = readLines("../updatedFakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d-T%H:%M:%S.000Z"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H:%M:%S")
data = rbind(data, temp)
}
data
data = readLines("../updatedFakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
str(data)
fake_json
data = readLines("../FakeData.json")
fake_json = lapply(data, fromJSON)
fake_json
##add fake information here first
data = readLines("../FakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d-T%H:%M:%S.000Z"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H:%M:%S")
data = rbind(data, temp)
}
data
data = readLines("../UpdatedFakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d %H:%M:%S"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H:%M:%S")
data = rbind(data, temp)
}
data
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE), cp = sample(c(1:20),nrow(data),replace=TRUE))
data
data[data$camera == 1, ]
data %>% group_by(date,time)
library(shiny)
library(dygraphs)
library(rgdal)
library(flexdashboard)
library(tidyverse)
library(plotly)
library(jsonlite)
library(DT)
library(jsonlite)
library(xts)
library(dplyr)
library(tmap)
##add fake information here first
data = readLines("../UpdatedFakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d %H:%M:%S"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H")
data = rbind(data, temp)
}
## Add count TRY ONLY
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE), cp = sample(c(1:20),nrow(data),replace=TRUE))
data
data %>% group_by(date,time)
data %>% group_by(date,time,cp)
data %>% group_by(date,time,cp)
summarise(hourly_count = sum(count))
data %>% group_by(id, camera, date,time,cp)
summarise(hourly_count = sum(count))
data %>% group_by(id, camera, date,time,cp)
summarize(hourly_count = sum(count))
data %>% group_by(id, camera, date,time,cp)
mutate(hourly_count = sum(count))
data %>% group_by(id, camera, date,time,cp)
summarise(hourly_count = n())
data %>% group_by(id, camera, date,time,cp)
data %>% group_by(id, camera, date,time,cp)
summarise(sum(count))
data %>% group_by(id, camera, date, time, cp)
summarise(count = sum(count))
data %>%
summarise(count = sum(count))
data %>% group_by(id, camera, date, time, cp)
data %>% group_by(id, camera, date, time, cp)
summarise(count = sum(count))
str(data)
sum(data$count)
data %>% group_by(id, camera, date, time, cp)
summarise(count = n())
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE), cp = sample(c(1:20),nrow(data),replace=TRUE)) %>% as_tibble()
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE), cp = sample(c(1:20),nrow(data),replace=TRUE)) %>% as.tibble()
##add fake information here first
data = readLines("../UpdatedFakeData.json")
fake_json = lapply(data, fromJSON)
data = data.frame(id = numeric(0), camera = numeric(0), time = character(0))
empty_table = data.frame()
for (i in 1:length(fake_json)) {
idx = i
occ = fake_json[[i]]$occurrence
temp  = data.frame(id = idx, camera = occ[1], time = occ[2])
temp$time = as.POSIXct(strptime(temp$time, format = "%Y-%m-%d %H:%M:%S"))
temp$date = as.Date(temp$time)
temp$time = format(as.POSIXct(temp$time), format = "%H") ## Just extract hours
data = rbind(data, temp)
}
data = cbind(data, count = sample(c(1:10), nrow(data), replace = TRUE), cp = sample(c(1:20),nrow(data),replace=TRUE)) %>% as.tibble()
data %>% group_by(id, camera, date, time, cp)
summarise(count = n())
data %>% group_by(id, camera, date, time, cp)
summarise(count = sum(count))
data %>% group_by(id, camera, date, time, cp)
summarise(count = sum(count))
data %>% group_by(id, camera, date, time, cp) %>% str()
data %>% group_by(id, camera, date, time, cp) %>%
summarise(count = sum(count))
data %>%
group_by(id, camera, date, time, cp) %>%
summarise(count = sum(count)) %>%
arrange(by_group=TRUE)
data %>%
group_by(id, camera, date, time, cp) %>%
summarise(count = sum(count)) %>%
arrange(by_group=TRUE) %>%
ungroup()
data <- data %>%
group_by(camera, date, time, cp) %>%
summarise(count = sum(count)) %>%
arrange(by_group=TRUE) %>%
ungroup()
data
install.packages("ComplexHeatMap")
install.packages("Rtools")
install.packages("ComplexHeatMap")
install.packages("Rtools")
sys.time()
install.packages("Sys.time")
